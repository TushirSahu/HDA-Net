{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2EE9qdg8tz3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2EE9qdg8tz3",
    "outputId": "580997d9-fb2d-4445-854c-10be10384d4e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "CV1mdn_axJ7D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CV1mdn_axJ7D",
    "outputId": "aff1f178-075d-4441-da63-0c916eeb4a85"
   },
   "outputs": [],
   "source": [
    "# !unzip '/content/drive/MyDrive/ddr/ddr.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1650c40",
   "metadata": {
    "id": "e1650c40"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "from tensorflow.keras import regularizers\n",
    "from keras import layers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import GlobalAveragePooling2D,Conv2D,Average,Reshape,Concatenate,Multiply,LayerNormalization,AveragePooling2D,GlobalMaxPool2D,Conv1D,BatchNormalization\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.applications.inception_v3 import InceptionV3 \n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications import ResNet50\n",
    "from keras.layers.core import Lambda\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.layers import Dense,Reshape,Activation,Permute,Dot,Dropout,ReLU,Add\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import *\n",
    "# from keras.utils import multi_gpu_model\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import load_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a96c3ec",
   "metadata": {
    "id": "3a96c3ec"
   },
   "outputs": [],
   "source": [
    "def Global_Attention_Block(inputs):\n",
    "        shape=K.int_shape(inputs)\n",
    "#         avg_pool=GlobalAveragePooling2D()(inputs)\n",
    "        avg_pool=AveragePooling2D(pool_size=(shape[1],shape[2])) (inputs)\n",
    "        avg_pool=Conv2D(shape[3],1,padding='same')(avg_pool)\n",
    "        avg_pool=Activation('sigmoid')(avg_pool)\n",
    "        avg_pool=Conv2D(shape[3],1,padding='same')(avg_pool)\n",
    "        avg_pool=Activation('sigmoid')(avg_pool)\n",
    "        \n",
    "        C_A= Multiply()([inputs,avg_pool])\n",
    "        avg_pool=Lambda(lambda x: K.mean(x,axis=-1,keepdims=True))(C_A)\n",
    "        avg_pool=Activation('sigmoid')(avg_pool)\n",
    "        S_A= Multiply()([avg_pool,C_A])\n",
    "        return S_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "xPR8YkbsPej4",
   "metadata": {
    "id": "xPR8YkbsPej4"
   },
   "outputs": [],
   "source": [
    "def ChannelAttention(inputs,ratio):\n",
    "    channels = inputs.shape[-1]\n",
    "    l1=Dense(channels//ratio,activation='relu',use_bias=False)\n",
    "    l2=Dense(channels,use_bias=False)\n",
    "\n",
    "\n",
    "    avg_pool = GlobalAveragePooling2D()(inputs)\n",
    "    avg_pool=l1(avg_pool)\n",
    "    avg_pool=l2(avg_pool)\n",
    "\n",
    "    max_pool=GlobalMaxPool2D()(inputs)\n",
    "    max_pool=l1(max_pool)\n",
    "    max_pool=l2(max_pool)\n",
    "\n",
    "    fc2 =max_pool+avg_pool\n",
    "    fc2 = Activation('sigmoid')(fc2)\n",
    "    attention = Multiply()([inputs, fc2])\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483d7b99",
   "metadata": {
    "id": "483d7b99"
   },
   "outputs": [],
   "source": [
    "def Category_Attention_Block(inputs,classes,k):\n",
    "    shape=K.int_shape(inputs)\n",
    "    F_1=Conv2D(k*classes,1,padding='same')(inputs)\n",
    "    F_1=BatchNormalization()(F_1)\n",
    "    F1=Activation('sigmoid')(F_1)\n",
    "    \n",
    "    F_2=F1\n",
    "    x=GlobalMaxPool2D()(F_2)\n",
    "    x=Reshape((classes,k)) (x)\n",
    "    S=Lambda(lambda x: K.mean(x,axis=-1,keepdims=False))(x)\n",
    "    \n",
    "    x=Reshape((shape[1],shape[2],classes,k)) (F1)\n",
    "    x=Lambda(lambda x: K.mean(x,axis=-1,keepdims=False))(x)\n",
    "    x=Multiply()([S,x])\n",
    "    M=Lambda(lambda x: K.mean(x,axis=-1,keepdims=True))(x)\n",
    "    \n",
    "    semantic=Multiply()([inputs,M])\n",
    "    return semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b95dfb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(inputs):\n",
    "    shape=K.int_shape(inputs)\n",
    "    x=AveragePooling2D(pool_size=(shape[1],shape[2])) (inputs)\n",
    "    x=Conv2D(shape[3]//4,(1,1),padding='same') (x)\n",
    "    x=Activation('relu') (x)\n",
    "    x=Conv2D(shape[3],(1,1), padding='same') (x)\n",
    "    x=Activation('sigmoid') (x)\n",
    "    C_A=Multiply()([x,inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "jvlEmK4Rbey1",
   "metadata": {
    "id": "jvlEmK4Rbey1"
   },
   "outputs": [],
   "source": [
    "def globalchannelattention(inputs):\n",
    "    shape=K.int_shape(inputs)\n",
    "    num_filters=shape[-1]\n",
    "  \n",
    "    initial=GlobalAveragePooling2D()(inputs)\n",
    "    initial=Reshape((1,num_filters))(initial)\n",
    "\n",
    "    a=Conv1D(num_filters,1,padding='same')(initial)\n",
    "    a=Activation('sigmoid')(a)\n",
    "\n",
    "    b=Conv1D(num_filters,1,padding='same')(initial)\n",
    "    b=Activation('sigmoid')(b)\n",
    "\n",
    "    b=K.permute_dimensions(b,(0,2,1))\n",
    "\n",
    "    out=K.batch_dot(b,a,axes=(2,1))\n",
    "    out=Activation('softmax')(out)\n",
    "\n",
    "    c=Reshape((shape[1]*shape[2],num_filters))(inputs)\n",
    "\n",
    "    final=K.batch_dot(c,out,axes=(2,1))\n",
    "\n",
    "    final=Reshape((shape[1],shape[2],shape[3]))(final)\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Jys1-J7N5kme",
   "metadata": {
    "id": "Jys1-J7N5kme"
   },
   "outputs": [],
   "source": [
    "def SelfAttention(inputs,ratio):\n",
    "   shape=K.int_shape(inputs)\n",
    "   num_filters=shape[-1]\n",
    "  #query\n",
    "   x_q=Conv2D(shape[-1]//ratio,(3,3),padding='same')(inputs)\n",
    "   x_q=Activation('sigmoid')(x_q)\n",
    "   x_qnew=Reshape((shape[1]*shape[2],shape[3]//ratio))(x_q)\n",
    "\n",
    "  #key\n",
    "   x_k=Conv2D(shape[-1]//ratio,(1,1),padding='same')(inputs)\n",
    "   x_k=Activation('sigmoid')(x_k)\n",
    "   x_knew=Reshape((shape[1]*shape[2],shape[3]//ratio))(x_k)\n",
    "  \n",
    "   x_knew=K.permute_dimensions(x_knew,(0,2,1))\n",
    "  #value\n",
    "   x_v=Conv2D(shape[-1]//ratio,(1,1),padding='same')(inputs)\n",
    "   x_v=Activation('softmax')(x_v)\n",
    "   x_vnew=Reshape((shape[1]*shape[2],shape[3]//ratio))(x_v)\n",
    "\n",
    "\n",
    "   x=K.batch_dot(x_qnew, x_knew,axes=(2,1))\n",
    "   x1=Activation('softmax')(x)\n",
    "    \n",
    "   x2=K.batch_dot(x1, x_vnew,axes=(2,1))\n",
    "\n",
    "   x_final=Reshape((shape[1],shape[2],shape[3]//ratio))(x2)\n",
    "\n",
    "   x_final=Conv2D(shape[-1],(1,1),padding='same')(x_final)\n",
    "\n",
    "   x_final=Activation('softmax')(x_final)\n",
    "\n",
    "   x_final=Add()([x_final,inputs])\n",
    "\n",
    "   return x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "LXy-hSA6BjnY",
   "metadata": {
    "id": "LXy-hSA6BjnY"
   },
   "outputs": [],
   "source": [
    "def rotateToAttend(input_feature):\n",
    "  shape = K.int_shape(input_feature)\n",
    "  # HxwxC\n",
    "  permute_1 =tf.keras.layers.Permute((3,2,1),input_shape=(shape[1],shape[2],shape[3]))(input_feature) \n",
    "  #  cxwxh\n",
    "  x1 = Lambda(lambda x: K.mean(x,axis=-1,keepdims=True))(permute_1)\n",
    "  x2 = Lambda(lambda x: K.max(x,axis=-1,keepdims=True))(permute_1)\n",
    "  x3 = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n",
    "  x = Conv2D(1,7, padding='same', dilation_rate=(1, 1)) (x3)\n",
    "  x = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.01)(x)\n",
    "  x = tf.keras.activations.sigmoid(x)  \n",
    "  x = tf.keras.layers.Multiply()([x,permute_1])\n",
    "  F1 = tf.keras.layers.Permute((3,2,1),input_shape=(shape[1],shape[2],shape[3]))(x)\n",
    "\n",
    "  permute_2 = tf.keras.layers.Permute((1,3,2),input_shape=(shape[1],shape[2],shape[3]))(input_feature)\n",
    "  x1 = Lambda(lambda x: K.mean(x,axis=-1,keepdims=True))(permute_2)\n",
    "  x2 = Lambda(lambda x: K.max(x,axis=-1,keepdims=True))(permute_2)\n",
    "  x3 = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n",
    "  x = Conv2D(1,7, padding='same', dilation_rate=(1, 1)) (x3)\n",
    "  x = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.01)(x)\n",
    "  x = tf.keras.activations.sigmoid(x)  \n",
    "  x = tf.keras.layers.Multiply()([x,permute_2])\n",
    "  F2 = tf.keras.layers.Permute((1,3,2),input_shape=(shape[1],shape[2],shape[3]))(x)\n",
    "\n",
    "\n",
    "  permute_3 = input_feature\n",
    "  x1 = Lambda(lambda x: K.mean(x,axis=-1,keepdims=True))(permute_3)\n",
    "  x2 = Lambda(lambda x: K.max(x,axis=-1,keepdims=True))(permute_3)\n",
    "  x3 = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n",
    "  x = Conv2D(1,7, padding='same', dilation_rate=(1, 1)) (x3)\n",
    "  x = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.01)(x)\n",
    "  x = tf.keras.activations.sigmoid(x)  \n",
    "  F3 = tf.keras.layers.Multiply()([x,permute_3])\n",
    "\n",
    "  attend_feature = tf.keras.layers.Average()([F1, F2, F3])\n",
    "  return attend_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8131d33",
   "metadata": {
    "id": "c8131d33"
   },
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4475ceba",
   "metadata": {
    "id": "4475ceba"
   },
   "outputs": [],
   "source": [
    "   \n",
    "def plotmodel(history,name):\n",
    "    \n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1) \n",
    "    \n",
    "    plt.figure(1)                  \n",
    "    plt.plot(epochs,smooth_curve(acc))\n",
    "    plt.plot(epochs,smooth_curve(val_acc))\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_acc', 'val_accuracy'], loc='upper left')\n",
    "    plt.title(name)\n",
    "    plt.savefig('acc_'+name+'.png')\n",
    "    \n",
    "    plt.figure(2)\n",
    "    plt.plot(epochs,smooth_curve(loss))\n",
    "    plt.plot(epochs,smooth_curve(val_loss))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_loss', 'val_loss'], loc='upper right')\n",
    "    plt.title(name)\n",
    "    plt.savefig('loss_'+name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89d1e574",
   "metadata": {
    "id": "89d1e574"
   },
   "outputs": [],
   "source": [
    "def get_base_model(model_name,img_size):\n",
    "    if(model_name=='vgg19'):\n",
    "        base_model=VGG19(include_top=False,weights='imagenet',input_shape=(img_size,img_size,3))\n",
    "    if model_name =='densenet121':\n",
    "        base_model=DenseNet121(include_top=False, weights='imagenet',input_shape=(image_size,image_size,3))\n",
    "    if(model_name=='inceptionv3'):\n",
    "        base_model=InceptionV3(include_top=False,weights='imagenet',input_shape=(img_size,img_size,3))\n",
    "    if(model_name=='resnet50'):\n",
    "        base_model=ResNet50(include_top=False,weights='imagenet',input_shape=(img_size,img_size,3))\n",
    "    if(model_name=='mobilenet'):\n",
    "        base_model=MobileNet(include_top=False,weights='imagenet',input_shape=(img_size,img_size,3))\n",
    "    if(model_name=='mobilenet1.0'):\n",
    "        base_model=MobileNet(include_top=False,weights='imagenet',alpha=1.0,input_shape=(img_size,img_size,3))\n",
    "    if(model_name=='xception'):\n",
    "        base_model=Xception(include_top=False,weights='imagenet',input_shape=(img_size,img_size,3))\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "AtTqXNdGDIeJ",
   "metadata": {
    "id": "AtTqXNdGDIeJ"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def weighted_cross_entropy(alpha=0.60,beta=0.60):\n",
    "#     def loss_function(y_true, y_pred):\n",
    "# #         weight = tf.where(tf.math.equal(y_true,3), alpha,tf.where(tf.math.equal(y_true,1),beta,1.0 - alpha))\n",
    "#         weight=tf.where(y_true==3,alpha,1.0-alpha)\n",
    "#         loss = CategoricalCrossentropy()(y_true, y_pred)\n",
    "#         weighted_loss = loss * weight\n",
    "#         return weighted_loss\n",
    "\n",
    "#     return loss_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffcd74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def weighted_categorical_crossentropy(class_weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        weights = K.constant(class_weights)\n",
    "        y_true = K.flatten(y_true)\n",
    "        y_pred = K.flatten(y_pred)\n",
    "        weights = K.gather(weights, K.cast(y_true, 'int32'))\n",
    "        loss = K.categorical_crossentropy(y_true, y_pred) * weights\n",
    "        return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee25f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_hierarchical(last3,last2,last1,ratio):\n",
    "    _,height3,width3,_=last3.shape\n",
    "    _,height2,width2,_=last2.shape\n",
    "    _,height1,width1,_=last1.shape\n",
    "\n",
    "    num_filters=last3.shape[-1]//ratio\n",
    "\n",
    "\n",
    "    input1=Conv2D(num_filters,(1,1),padding='same')(last3)  #h1xw1xc1\n",
    "    input2=Conv2D(num_filters,(1,1),padding='same')(last2)  #h2xw2xc2\n",
    "    input3=Conv2D(num_filters,(1,1),padding='same')(last1)  #h3xw3xc3\n",
    "\n",
    "    inp1=Reshape((height3*width3,num_filters))(input1) #h1w1xc\n",
    "\n",
    "    inp2=Reshape((height2*width2,num_filters))(input2) #h2w2xc\n",
    "\n",
    "    inp3=Reshape((height1*width1,num_filters))(input3) #h3w3xc\n",
    "    \n",
    "\n",
    "    k2=K.permute_dimensions(inp2,(0,2,1))  #cxh2w2\n",
    "\n",
    "    x=K.batch_dot(inp1,k2,axes=(2,1)) #h1w1xh2w2\n",
    "    x=Activation('softmax')(x)\n",
    "\n",
    "    x1=K.batch_dot(x,inp2,axes=(2,1)) #h1w1xc\n",
    "    x1=Reshape((height3,width3,num_filters))(x1) #h1xw1xc\n",
    "\n",
    "    k3=K.permute_dimensions(inp3,(0,2,1)) #cxh3w3\n",
    "\n",
    "    x2=K.batch_dot(inp1,k3,axes=(2,1)) #h1w1xh3w3\n",
    "    x2=Activation('softmax')(x2)\n",
    "    \n",
    "    x2=K.batch_dot(x2,inp3,axes=(2,1)) #h1w1xc\n",
    "    x2=Reshape((height3,width3,num_filters))(x2) #h1xw1xc\n",
    "    #x_final=Average()([x1,x2,input1])\n",
    "\n",
    "    x_final=Concatenate()([x1,x2,last3])\n",
    "#     x_final=Activation('softmax')(x_final)\n",
    "    x_final=Conv2D(last3.shape[-1],(1,1),padding='same')(x_final)\n",
    "\n",
    "    return x_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88f5990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model(last3,last2,last1):\n",
    "    _,height,width,channels=last3.shape\n",
    "    input1=Reshape((height*width,last3.shape[-1]))(last3) #h1w1xc1\n",
    "\n",
    "    input2=Reshape((height*width,last2.shape[-1]))(last2) #h1w1xc2\n",
    "\n",
    "    input3=Reshape((height*width,last1.shape[-1]))(last1) #h1w1xc3\n",
    "\n",
    "    k2=K.permute_dimensions(input2,(0,2,1)) # c2xh1w1\n",
    "\n",
    "    x=K.batch_dot(k2,input1,axes=(2,1))  #c2xc1\n",
    "    x=Activation('softmax')(x)\n",
    "\n",
    "    x1=K.batch_dot(input2, x,axes=(2,1)) #h1w1xc1\n",
    "    x1=Reshape((height3,width3,channels))(x1)\n",
    "\n",
    "    k3=K.permute_dimensions(input3,(0,2,1)) # c3xh3w3\n",
    "    \n",
    "    x2=K.batch_dot(k3,input1,axes=(2,1)) #c3xc1\n",
    "    x2=Activation('softmax')(x2)\n",
    "\n",
    "    x2=K.batch_dot(input3,x2,axes=(2,1)) #h1w1xc1\n",
    "    x2=Reshape((height3,width3,channels))(x2)\n",
    "\n",
    "\n",
    "    x_final=Concatenate()([x1,x2,last3])\n",
    "#     x_final=Activation('softmax')(x_final)\n",
    "    x_final=Conv2D(channels,(1,1),padding='same')(x_final)\n",
    "   \n",
    "   \n",
    "    return x_final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395845ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dir_fold = '/content/ddr/train'\n",
    "val_dir_fold = '/content/ddr/validation'\n",
    "\n",
    "train_num, valid_num = 6260, 2503\n",
    "\n",
    "train = ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "\n",
    "valid = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train.flow_from_directory(train_dir_fold, target_size=(image_size, image_size), shuffle=True,\n",
    "                                           batch_size=batch_size)\n",
    "val_data = valid.flow_from_directory(val_dir_fold, target_size=(image_size, image_size), shuffle=False,\n",
    "                                         batch_size=batch_size)\n",
    "\n",
    "lr_decay = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, verbose=1)\n",
    "save_model = ModelCheckpoint('new/' + save_name + '{epoch:02d}.h5', monitor='val_loss', save_best_only=True,\n",
    "                                 mode='min')\n",
    "\n",
    "classes = np.unique(train_data.classes)\n",
    "class_counts = np.sum(to_categorical(train_data.classes, num_classes=len(classes)), axis=0)\n",
    "total_samples = np.sum(class_counts)\n",
    "class_weights = total_samples / (len(classes) * class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7edc9257",
   "metadata": {
    "id": "7edc9257"
   },
   "outputs": [],
   "source": [
    "def train_model(model, image_size, batch_size, save_name, lr1, lr2, Epochs1, Epochs2):\n",
    "\n",
    "    train_dir_fold = '/content/ddr/train'\n",
    "    val_dir_fold = '/content/ddr/validation'\n",
    "\n",
    "    train_num, valid_num = 6260, 2503\n",
    "\n",
    "    train = ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "\n",
    "    valid = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_data = train.flow_from_directory(train_dir_fold, target_size=(image_size, image_size), shuffle=True,\n",
    "                                           batch_size=batch_size)\n",
    "    val_data = valid.flow_from_directory(val_dir_fold, target_size=(image_size, image_size), shuffle=False,\n",
    "                                         batch_size=batch_size)\n",
    "\n",
    "    lr_decay = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, verbose=1)\n",
    "    save_model = ModelCheckpoint('new/' + save_name + '{epoch:02d}.h5', monitor='val_loss', save_best_only=True,\n",
    "                                 mode='min')\n",
    "\n",
    "    classes = np.unique(train_data.classes)\n",
    "    class_counts = np.sum(to_categorical(train_data.classes, num_classes=len(classes)), axis=0)\n",
    "    total_samples = np.sum(class_counts)\n",
    "    class_weights = total_samples / (len(classes) * class_counts)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=lr1, decay=0.00001), loss=weighted_categorical_crossentropy(class_weights), metrics=['accuracy'])\n",
    "    model.fit(train_data, steps_per_epoch=train_num/batch_size, validation_data=val_data,\n",
    "              validation_steps=valid_num/batch_size, epochs=Epochs1, workers=2,\n",
    "              callbacks=[lr_decay, save_model])\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=lr2, decay=0.00001), loss=weighted_categorical_crossentropy(class_weights), metrics=['accuracy'])\n",
    "    history = model.fit(train_data, steps_per_epoch=train_num/batch_size,\n",
    "                        validation_data=val_data, validation_steps=valid_num/batch_size, epochs=Epochs2, workers=2,\n",
    "                        callbacks=[lr_decay, save_model])\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50a26fb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50a26fb8",
    "outputId": "665a44d8-bfa7-44e2-a8c1-9190784f1f9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 18:31:25.637783: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-06-08 18:31:25.640953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-06-08 18:31:25.740233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-06-08 18:31:25.740870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-06-08 18:31:25.740894: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-08 18:31:25.743430: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-08 18:31:25.743511: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-06-08 18:31:25.745257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-08 18:31:25.745576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-08 18:31:25.747870: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-06-08 18:31:25.749046: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-06-08 18:31:25.753403: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-08 18:31:25.755827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2023-06-08 18:31:25.757202: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-08 18:31:25.759243: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-06-08 18:31:25.995525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-06-08 18:31:25.996117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-06-08 18:31:25.996148: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-08 18:31:25.996185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-08 18:31:25.996198: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-06-08 18:31:25.996211: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-08 18:31:25.996222: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-08 18:31:25.996235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-06-08 18:31:25.996246: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-06-08 18:31:25.996260: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-08 18:31:25.998370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2023-06-08 18:31:25.998416: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-08 18:31:27.057334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-06-08 18:31:27.057374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2023-06-08 18:31:27.057382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N \n",
      "2023-06-08 18:31:27.057387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N \n",
      "2023-06-08 18:31:27.059951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10070 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:18:00.0, compute capability: 7.5)\n",
      "2023-06-08 18:31:27.061645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10070 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3b:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 396, 396, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 402, 402, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 198, 198, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 198, 198, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 198, 198, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 200, 200, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 99, 99, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 99, 99, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 99, 99, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 99, 99, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 99, 99, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 99, 99, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 99, 99, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 99, 99, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 99, 99, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 99, 99, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 99, 99, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 99, 99, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 99, 99, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 99, 99, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 99, 99, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 99, 99, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 99, 99, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 99, 99, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 99, 99, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 99, 99, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 99, 99, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 99, 99, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 99, 99, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 99, 99, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 99, 99, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 99, 99, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 99, 99, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 99, 99, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 99, 99, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 99, 99, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 99, 99, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 99, 99, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 99, 99, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 50, 50, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 50, 50, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 50, 50, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 50, 50, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 50, 50, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 50, 50, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 50, 50, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 50, 50, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 50, 50, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 50, 50, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 50, 50, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 50, 50, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 50, 50, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 50, 50, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 50, 50, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 50, 50, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 50, 50, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 50, 50, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 50, 50, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 50, 50, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 50, 50, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 50, 50, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 50, 50, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 50, 50, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 50, 50, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 50, 50, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 50, 50, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 50, 50, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 50, 50, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 50, 50, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 50, 50, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 50, 50, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 50, 50, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 50, 50, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 50, 50, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 50, 50, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 50, 50, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 50, 50, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 50, 50, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 50, 50, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 50, 50, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 50, 50, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 25, 25, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 25, 25, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 25, 25, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 25, 25, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 25, 25, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 25, 25, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 25, 25, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 25, 25, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 25, 25, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 25, 25, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 25, 25, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 25, 25, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 25, 25, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 25, 25, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 25, 25, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 25, 25, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 25, 25, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 25, 25, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 25, 25, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 25, 25, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 25, 25, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 25, 25, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 25, 25, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 25, 25, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 25, 25, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 25, 25, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 25, 25, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 25, 25, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 25, 25, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 25, 25, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 25, 25, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 25, 25, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 25, 25, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 25, 25, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 25, 25, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 25, 25, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 25, 25, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 25, 25, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 25, 25, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 25, 25, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 25, 25, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 25, 25, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 25, 25, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 25, 25, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 25, 25, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 25, 25, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 25, 25, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 25, 25, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 25, 25, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 25, 25, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 25, 25, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 25, 25, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 25, 25, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 25, 25, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 25, 25, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 25, 25, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 25, 25, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 25, 25, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 25, 25, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 25, 25, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 25, 25, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 25, 25, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 13, 13, 512)  524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 13, 13, 512)  2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 13, 13, 512)  0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 13, 13, 512)  2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 13, 13, 512)  2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 13, 13, 512)  0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 13, 13, 2048) 2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 13, 13, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 13, 13, 2048) 8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 13, 13, 2048) 8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 13, 13, 2048) 0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 13, 13, 2048) 0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 13, 13, 512)  1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 13, 13, 512)  2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 13, 13, 512)  0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 13, 13, 512)  2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 13, 13, 512)  2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 13, 13, 512)  0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 13, 13, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 13, 13, 2048) 8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 13, 13, 2048) 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 13, 13, 2048) 0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 13, 13, 512)  1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 13, 13, 512)  2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 13, 13, 512)  0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 13, 13, 512)  2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 13, 13, 512)  2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 13, 13, 512)  0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 13, 13, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 13, 13, 2048) 8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 13, 13, 2048) 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 13, 13, 2048) 0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 13, 13, 1024) 2098176     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 13, 13, 1024) 2098176     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 13, 13, 1024) 2098176     conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 169, 1024)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 169, 1024)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 169, 1024)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose (TFOpLam (None, 1024, 169)    0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_1 (TFOpL (None, 1024, 169)    0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.linalg.matmul (TFOpLambda)   (None, 169, 169)     0           reshape[0][0]                    \n",
      "                                                                 tf.compat.v1.transpose[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.linalg.matmul_2 (TFOpLambda) (None, 169, 169)     0           reshape[0][0]                    \n",
      "                                                                 tf.compat.v1.transpose_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 169, 169)     0           tf.linalg.matmul[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 169, 169)     0           tf.linalg.matmul_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.linalg.matmul_1 (TFOpLambda) (None, 169, 1024)    0           activation[0][0]                 \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.linalg.matmul_3 (TFOpLambda) (None, 169, 1024)    0           activation_1[0][0]               \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 13, 13, 1024) 0           tf.linalg.matmul_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 13, 13, 1024) 0           tf.linalg.matmul_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 13, 13, 4096) 0           reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 13, 13, 2048) 8390656     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 5)            10245       global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 38,283,141\n",
      "Trainable params: 38,230,021\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D, Dropout, Dense\n",
    "\n",
    "k = 5\n",
    "lr1 = 0.004\n",
    "lr2 = 0.0001\n",
    "batch_size = 16\n",
    "image_size = 396\n",
    "classes = 5\n",
    "\n",
    "\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(image_size, image_size, 3))\n",
    "base_in = base_model.input\n",
    "base_out=base_model.output\n",
    "\n",
    "\n",
    "last3 = base_model.get_layer('conv5_block3_out').output\n",
    "last2 = base_model.get_layer('conv5_block2_out').output\n",
    "last1 = base_model.get_layer('conv5_block1_out').output\n",
    "# print(last)\n",
    "\n",
    "# last3 = Conv2D(int(last3.shape[-1]) // 2, (1, 1), padding='same')(last3)\n",
    "# last2 = Conv2D(int(last2.shape[-1]) // 2, (1, 1), padding='same')(last2)\n",
    "# last1 = Conv2D(int(last1.shape[-1]) // 2, (1, 1), padding='same')(last1)\n",
    "\n",
    "# x1 = globalchannelattention(compress)\n",
    "# x2 = SelfAttention(compress, 2)\n",
    "# x = tf.keras.layers.Add()([x1, x2])\n",
    "# dense_block1 = base_model.get_layer('conv5_block14_concat').output\n",
    "# dense_block2 = base_model.get_layer('conv5_block15_concat').output\n",
    "# dense_block3 = base_model.get_layer('conv5_block16_concat').output\n",
    "\n",
    "# x1=new_model(last3,last2,last1)\n",
    "x2=spatial_hierarchical(last3,last2,last1,2)\n",
    "\n",
    "\n",
    "# x=Concatenate()([x1,x2])\n",
    "# print(x.shape)\n",
    "\n",
    "\n",
    "x = GlobalAveragePooling2D()(x2)\n",
    "# x = Dropout(0.1)(x)\n",
    "out = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "parallel_model = Model(base_in, out)\n",
    "parallel_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075496dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "075496dd",
    "outputId": "b085b717-c3d4-4469-a0cb-47b20750a624",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6260 images belonging to 5 classes.\n",
      "Found 2503 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 18:31:33.596778: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-06-08 18:31:33.614718: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz\n",
      "2023-06-08 18:31:36.581283: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-08 18:31:37.413738: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-08 18:31:38.967583: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2023-06-08 18:31:39.080391: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/391 [=======>......................] - ETA: 2:47 - loss: 92.0460 - accuracy: 0.3501"
     ]
    }
   ],
   "source": [
    "history=train_model(parallel_model,image_size,batch_size,'resnet_new_',lr1,lr2,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smyEy7KklWjd",
   "metadata": {
    "id": "smyEy7KklWjd"
   },
   "outputs": [],
   "source": [
    "plotmodel(history,'resnet_new_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j8G2Jd4GdB0h",
   "metadata": {
    "id": "j8G2Jd4GdB0h"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "loss_function=weighted_cross_entropy()\n",
    "# keras.utils.register_keras_serializable('loss_function')(loss_function)\n",
    "\n",
    "with keras.utils.custom_object_scope({'loss_function':loss_function}):\n",
    "         model_name='/home/dipankar/dipankar/tushir/new/resnet_new_05.h5'\n",
    "         model = load_model(model_name)\n",
    "\n",
    "test_dir = '/home/dipankar/dipankar/tushir/ddr/test/' \n",
    "batch_size = 16\n",
    "image_size=448\n",
    "test_data = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_data)\n",
    "\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "y_true = test_data.classes\n",
    "y_pred = model.predict(test_data)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n",
    "print(confusion_mtx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(confusion_mtx,annot=True,fmt='d',cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_kappa(result,test_num):\n",
    "    weight=np.zeros((5,5),dtype='float')\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            weight[i,j]=(i-j)*(i-j)/16\n",
    "    fenzi=0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            fenzi=fenzi+result[i,j]*weight[i,j]\n",
    "    fenmu=0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            fenmu=fenmu+weight[i,j]*result[:,j].sum()*result[i,:].sum()\n",
    "\n",
    "    weght_kappa=1-(fenzi/(fenmu/test_num))\n",
    "    return  weght_kappa\n",
    "\n",
    "\n",
    "test_num=0\n",
    "result=np.zeros((5,5),dtype=int)\n",
    "recall=np.zeros((1,5),dtype=float)\n",
    "for i in range(5):\n",
    "        datadirs=test_dir+str(i)+'/'\n",
    "        filenames=os.listdir(datadirs)\n",
    "        num=len(filenames)\n",
    "        test_num=test_num+num\n",
    "        valid = ImageDataGenerator(rescale=1./255)\n",
    "        valid_data=valid.flow_from_directory(directory=test_dir,target_size=(image_size,image_size),\n",
    "                                             batch_size=batch_size,class_mode=None,classes=str(i))\n",
    "        predict=model.predict_generator(valid_data,steps=num/batch_size,verbose=1,workers=1)\n",
    "        predict=np.argmax(predict,axis=-1)\n",
    "        for j in range(5):\n",
    "            result[i,j]=np.sum(predict==j)\n",
    "\n",
    "right=result[0,0]+result[1,1]+result[2,2]+result[3,3]+result[4,4]\n",
    "print('Acc:',right/test_num)\n",
    "\n",
    "w_kappa=weight_kappa(result,test_num)\n",
    "print('w_kappa:',w_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RD11r5fCGlZf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "RD11r5fCGlZf",
    "outputId": "1d446e7d-4981-47b2-954c-efc4de3fc528"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def grad_cam(model, image_path, layer_name, class_index, image_size):\n",
    "    img = image.load_img(image_path, target_size=(image_size, image_size))\n",
    "    img_orig = img.copy()\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    grad_model = Model(inputs=model.input, outputs=(model.get_layer(layer_name).output, model.output))\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img)\n",
    "        loss = predictions[:, class_index]\n",
    "\n",
    "    output = conv_outputs[0]\n",
    "    grads = tape.gradient(loss, conv_outputs)[0]\n",
    "\n",
    "    weights = tf.reduce_mean(grads, axis=(0, 1))\n",
    "    cam = np.dot(output, weights)\n",
    "\n",
    "    cam = cv2.resize(cam, (image_size, image_size))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    img_orig = np.array(img_orig) \n",
    "    overlaid_img = cv2.addWeighted(img_orig, 0.7, heatmap, 0.3, 0)\n",
    "\n",
    "    return overlaid_img\n",
    "\n",
    "\n",
    "image_path = '/home/dipankar/dipankar/tushir/ddr/train/3/007-2766-100.jpg'  \n",
    "layer_name = 'reshape_2'  \n",
    "class_index =4\n",
    "\n",
    "image_size = 512 \n",
    "\n",
    "overlaid_img  = grad_cam(parallel_model, image_path, layer_name, class_index, image_size)\n",
    "original_img = image.load_img(image_path, target_size=(image_size, image_size))\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(original_img)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Original Image')\n",
    "axs[1].imshow(overlaid_img)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Overlaid Image with Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08477263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "c268c1f4b20d46e39df77955af542fe69fc17facdbea48218bd0d6964daf30ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
